{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkGRzyG0xkjET22BqXMKkc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/achalbajpai/llm-scratch/blob/main/LLMsFromScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ib-eIsYGd221"
      },
      "source": [
        "## Step 1: Creating Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "The print command prints the total number of characters followed by the first 100\n",
        "characters of this file for illustration purposes. </div>"
      ],
      "metadata": {
        "id": "awnV7fdmkqw4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrQmKG-dV6E2",
        "outputId": "721b961e-9093-4615-c8c4-3007fb9f027b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of characters: 4348\n",
            "Cristiano Ronaldo dos Santos Aveiro, born on 5 February 1985, is a Portuguese professional football\n"
          ]
        }
      ],
      "source": [
        "# Open the file \"cristiano-ronaldo.txt\" in read mode with UTF-8 encoding\n",
        "# The `with` statement ensures the file is properly closed after reading\n",
        "with open(\"cr7.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    # Read the entire content of the file into the variable `raw_text`\n",
        "    raw_text = f.read()\n",
        "\n",
        "# Print the total number of characters in the file\n",
        "# `len(raw_text)` returns the length of the string `raw_text`, which is the number of characters\n",
        "print(\"Total number of characters:\", len(raw_text))\n",
        "\n",
        "# Print the first 99 characters of the file\n",
        "# `raw_text[:99]` slices the string `raw_text` to get the first 99 characters\n",
        "print(raw_text[:99])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer\n"
      ],
      "metadata": {
        "id": "bpU_tvTHfelh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Our goal is to tokenize this 4348-character short story into individual words and special\n",
        "characters that we can then turn into embeddings for LLM training  </div>"
      ],
      "metadata": {
        "id": "rSQyI89-lu9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "Note that it's common to process millions of articles and hundreds of thousands of\n",
        "books -- many gigabytes of text -- when working with LLMs. However, for educational\n",
        "purposes, it's sufficient to work with smaller text samples like a single book to\n",
        "illustrate the main ideas behind the text processing steps and to make it possible to\n",
        "run it in reasonable time on consumer hardware. </div>"
      ],
      "metadata": {
        "id": "6Md5PglPlz3D"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEO8kkKKuAXU"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "How can we best split this text to obtain a list of tokens? For this, we go on a small\n",
        "excursion and use Python's regular expression library re for illustration purposes. (Note\n",
        "that you don't have to learn or memorize any regular expression syntax since we will\n",
        "transition to a pre-built tokenizer later in this chapter.) </div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the `re` module, which provides support for regular expressions in Python\n",
        "import re\n",
        "\n",
        "# Define a sample text string to be processed\n",
        "text = \"Hello, world. This, is a test.\"\n",
        "\n",
        "# Use the `re.split()` function to split the text into tokens and separators\n",
        "# The regular expression `([,.:;?_!\"()\\']|--|\\s)` matches:\n",
        "# - Punctuation marks: , . : ; ? _ ! \" ( ) '\n",
        "# - Double hyphens: --\n",
        "# - Whitespace: \\s\n",
        "# The parentheses `()` around the regex ensure that the separators are included in the result\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "# Print the initial result of the split\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsjZSf8vd3hr",
        "outputId": "44f0ab64-7eb6-4127-85b6-03eb41505875"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxfS3g4auAXV"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "The result is a list of individual words, whitespaces, and punctuation characters:\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tp0K39xRuAXV"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "Let's modify the regular expression splits on whitespaces (\\s) and commas, and periods\n",
        "([,.]):</div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean the result list:\n",
        "# 1. Use a list comprehension to iterate over each item in `result`\n",
        "# 2. Apply `item.strip()` to remove leading and trailing whitespace from each item\n",
        "# 3. Filter out any empty strings using `if item.strip()`\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "\n",
        "# Print the cleaned result\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhdSGJCP6KTj",
        "outputId": "a2b8df95-e221-497e-de42-e36dbb3ebaa2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aF-KroyWuAXV"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "REMOVING WHITESPACES OR NOT\n",
        "\n",
        "\n",
        "When developing a simple tokenizer, whether we should encode whitespaces as\n",
        "separate characters or just remove them depends on our application and its\n",
        "requirements. Removing whitespaces reduces the memory and computing\n",
        "requirements. However, keeping whitespaces can be useful if we train models that\n",
        "are sensitive to the exact structure of the text (for example, Python code, which is\n",
        "sensitive to indentation and spacing). Here, we remove whitespaces for simplicity\n",
        "and brevity of the tokenized outputs. Later, we will switch to a tokenization scheme\n",
        "that includes whitespaces.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE9nek4auAXW"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "The tokenization scheme we devised above works well on the simple sample text. Let's\n",
        "modify it a bit further so that it can also handle other types of punctuation, such as\n",
        "question marks, quotation marks, and the double-dashes we have seen earlier in the first\n",
        "100 characters of Edith Wharton's short story, along with additional special characters: </div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, world. Is this-- a test?\"\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3K5rRFXV66Dz",
        "outputId": "1cd2d467-8aff-4ad8-ec36-38dba7040481"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternative approach (redundant in this case, as the previous step already handles this):\n",
        "# Filter out any empty strings from the list\n",
        "# This step is unnecessary here because the previous list comprehension already removes empty strings\n",
        "result = [item for item in result if item.strip()]\n",
        "\n",
        "# Print the final result\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFFiCqMy6MmK",
        "outputId": "44e74597-305d-4203-9262-0e5031f3fb40"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the `re.split()` function to split the `raw_text` string into tokens and separators\n",
        "# The regular expression `([,.:;?_!\"()\\']|--|\\s)` matches:\n",
        "# - Punctuation marks: , . : ; ? _ ! \" ( ) '\n",
        "# - Double hyphens: --\n",
        "# - Whitespace: \\s\n",
        "# The parentheses `()` around the regex ensure that the separators are included in the result\n",
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "\n",
        "# Clean the `preprocessed` list:\n",
        "# 1. Use a list comprehension to iterate over each item in `preprocessed`\n",
        "# 2. Apply `item.strip()` to remove leading and trailing whitespace from each item\n",
        "# 3. Filter out any empty strings using `if item.strip()`\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "\n",
        "# Print the first 30 items of the cleaned `preprocessed` list\n",
        "# This is useful for inspecting the initial part of the processed data\n",
        "print(preprocessed[:30])\n",
        "\n",
        "# Print the total number of items in the cleaned `preprocessed` list\n",
        "# This gives an idea of the size of the processed data\n",
        "print(len(preprocessed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkIWCZXFfqzF",
        "outputId": "4acd3aa2-7e30-436b-e103-37dc92455e3b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Cristiano', 'Ronaldo', 'dos', 'Santos', 'Aveiro', ',', 'born', 'on', '5', 'February', '1985', ',', 'is', 'a', 'Portuguese', 'professional', 'footballer', '.', 'He', 'plays', 'as', 'a', 'forward', 'for', 'and', 'captains', 'both', 'the', 'Saudi', 'Pro']\n",
            "882\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2 - Token to Token ID**\n"
      ],
      "metadata": {
        "id": "QD-LNBrRgidF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WddQ3eSuAXW"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "In the previous section, we tokenized CR7 short story and assigned it to a\n",
        "Python variable called preprocessed. Let's now create a list of all unique tokens and sort\n",
        "them alphabetically to determine the vocabulary size:</div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sorted list of unique words (tokens) from the `preprocessed` list\n",
        "# 1. `set(preprocessed)` creates a set of unique items from `preprocessed`.\n",
        "# 2. `sorted()` sorts the unique items alphabetically.\n",
        "all_words = sorted(set(preprocessed))\n",
        "\n",
        "# Calculate the size of the vocabulary (number of unique words)\n",
        "# `len(all_words)` returns the number of items in the `all_words` list\n",
        "vocab_size = len(all_words)\n",
        "\n",
        "# Print the size of the vocabulary\n",
        "# This gives the total number of unique words in the preprocessed data\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwYfMtxBgpvs",
        "outputId": "62de1830-afb1-40a9-c9d2-4fa44f2d99db"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fPvirHiuAXX"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "After determining that the vocabulary size is 303 via the above code, we create the\n",
        "vocabulary and print its first 51 entries for illustration purposes:\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a vocabulary dictionary that maps each unique token to a unique integer\n",
        "# 1. `enumerate(all_words)` generates pairs of (index, token) for each token in `all_words`.\n",
        "#    - `index` is the position of the token in the sorted list (starting from 0).\n",
        "#    - `token` is the word itself.\n",
        "# 2. A dictionary comprehension is used to create the `vocab` dictionary:\n",
        "#    - The key is the `token` (word).\n",
        "#    - The value is the `integer` (index).\n",
        "vocab = {token: integer for integer, token in enumerate(all_words)}"
      ],
      "metadata": {
        "id": "KzZF8CuBhsWH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the items in the `vocab` dictionary using `enumerate`\n",
        "# `enumerate(vocab.items())` generates pairs of (index, (token, integer)) for each item in `vocab`\n",
        "# - `index` is the position of the item in the iteration (starting from 0).\n",
        "# - `item` is a tuple of (token, integer), where:\n",
        "#   - `token` is the word (key in the dictionary).\n",
        "#   - `integer` is the corresponding ID (value in the dictionary).\n",
        "for i, item in enumerate(vocab.items()):\n",
        "    # Print the current item (tuple of (token, integer))\n",
        "    print(item)\n",
        "\n",
        "    # Stop the loop after printing 50 items\n",
        "    # This is useful for inspecting the first few items in a large dictionary\n",
        "    if i >= 50:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JraDFZHjh2L1",
        "outputId": "cbd1e09e-987d-4f65-d25b-82d6948a2281"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"'\", 0)\n",
            "('(', 1)\n",
            "(')', 2)\n",
            "(',', 3)\n",
            "('.', 4)\n",
            "('1', 5)\n",
            "('100', 6)\n",
            "('135', 7)\n",
            "('14', 8)\n",
            "('140', 9)\n",
            "('18', 10)\n",
            "('183', 11)\n",
            "('1985', 12)\n",
            "('200', 13)\n",
            "('2003', 14)\n",
            "('2004', 15)\n",
            "('2008', 16)\n",
            "('2009', 17)\n",
            "('2013', 18)\n",
            "('2014', 19)\n",
            "('2015', 20)\n",
            "('2016', 21)\n",
            "('2017', 22)\n",
            "('2018', 23)\n",
            "('2019', 24)\n",
            "('2020', 25)\n",
            "('2021', 26)\n",
            "('2022', 27)\n",
            "('2023', 28)\n",
            "('2024', 29)\n",
            "('217', 30)\n",
            "('23', 31)\n",
            "('30', 32)\n",
            "('33', 33)\n",
            "('42', 34)\n",
            "('5', 35)\n",
            "('8', 36)\n",
            "('900', 37)\n",
            "('A', 38)\n",
            "('Additionally', 39)\n",
            "('Al', 40)\n",
            "('At', 41)\n",
            "('Aveiro', 42)\n",
            "('Awards', 43)\n",
            "('Bale', 44)\n",
            "('Ballon', 45)\n",
            "('Ballons', 46)\n",
            "('Benzema', 47)\n",
            "('Boot', 48)\n",
            "('CP', 49)\n",
            "('Champions', 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dap4nh8NuAXX"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "As we can see, based on the output above, the dictionary contains individual tokens\n",
        "associated with unique integer labels.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zx_cDWI4uAXX"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Later in this book, when we want to convert the outputs of an LLM from numbers back into\n",
        "text, we also need a way to turn token IDs into text.\n",
        "\n",
        "For this, we can create an inverse\n",
        "version of the vocabulary that maps token IDs back to corresponding text tokens.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhMXFJ0VuAXX"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Let's implement a complete tokenizer class in Python.\n",
        "\n",
        "The class will have an encode method that splits\n",
        "text into tokens and carries out the string-to-integer mapping to produce token IDs via the\n",
        "vocabulary.\n",
        "\n",
        "In addition, we implement a decode method that carries out the reverse\n",
        "integer-to-string mapping to convert the token IDs back into text.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XxyLOWQuAXX"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    \n",
        "Step 1: Store the vocabulary as a class attribute for access in the encode and decode methods\n",
        "    \n",
        "Step 2: Create an inverse vocabulary that maps token IDs back to the original text tokens\n",
        "\n",
        "Step 3: Process input text into token IDs\n",
        "\n",
        "Step 4: Convert token IDs back into text\n",
        "\n",
        "Step 5: Replace spaces before the specified punctuation\n",
        "\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        \"\"\"\n",
        "        Initialize the tokenizer with a vocabulary mapping.\n",
        "\n",
        "        Args:\n",
        "            vocab (dict): A dictionary mapping tokens (strings) to unique integers (IDs).\n",
        "        \"\"\"\n",
        "        # Map tokens to IDs (e.g., {\"Hello\": 1, \"world\": 2})\n",
        "        self.str_to_int = vocab\n",
        "\n",
        "        # Create the inverse mapping: IDs to tokens (e.g., {1: \"Hello\", 2: \"world\"})\n",
        "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"\n",
        "        Convert a text string into a list of token IDs.\n",
        "\n",
        "        Args:\n",
        "            text (str): The input text to encode.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of token IDs corresponding to the input text.\n",
        "        \"\"\"\n",
        "        import re\n",
        "\n",
        "        # Step 1: Preprocess the text\n",
        "        # Split the text into tokens and separators (like punctuation and spaces)\n",
        "        # using a regular expression.\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "        # Step 2: Clean the preprocessed list\n",
        "        # Remove leading/trailing spaces from each item and filter out empty strings.\n",
        "        preprocessed = [\n",
        "            item.strip() for item in preprocessed if item.strip()\n",
        "        ]\n",
        "\n",
        "        # Step 3: Convert tokens to IDs\n",
        "        # Use the str_to_int mapping to convert each token to its corresponding ID.\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "\n",
        "        # Return the list of token IDs\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        \"\"\"\n",
        "        Convert a list of token IDs back into a text string.\n",
        "\n",
        "        Args:\n",
        "            ids (list): A list of token IDs to decode.\n",
        "\n",
        "        Returns:\n",
        "            str: The decoded text string.\n",
        "        \"\"\"\n",
        "        # Step 1: Convert IDs back to tokens\n",
        "        # Use the int_to_str mapping to convert each ID to its corresponding token.\n",
        "        tokens = [self.int_to_str[i] for i in ids]\n",
        "\n",
        "        # Step 2: Join tokens into a single string with spaces in between\n",
        "        text = \" \".join(tokens)\n",
        "\n",
        "        # Step 3: Postprocess the text\n",
        "        # Fix spacing issues around punctuation marks (e.g., \"Hello , world !\" -> \"Hello, world!\")\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "\n",
        "        # Return the decoded text\n",
        "        return text"
      ],
      "metadata": {
        "id": "L83OMlkWeHwI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlByjd-EuAXX"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Let's instantiate a new tokenizer object from the SimpleTokenizerV1 class and tokenize a\n",
        "passage from CR7 short story to try it out in practice:\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "# Input text\n",
        "text = \"\"\"Ronaldo has won five Ballon d'Or awards, a record three UEFA Men's Player of the Year Awards, and four European Golden Shoes. He was named the world's best player by FIFA five times, the most by a European player. He has won 33 trophies in his career, including seven league titles, five UEFA Champions Leagues, the UEFA European Championship, and the UEFA Nations League.\"\"\"\n",
        "\n",
        "# Encode the text\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "id": "I8OP_UplebPg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b868f8cd-7faf-44ce-f58a-eb6366299503"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100, 191, 296, 177, 45, 163, 0, 92, 144, 3, 123, 249, 273, 114, 88, 0, 253, 93, 230, 268, 121, 43, 3, 134, 184, 63, 72, 104, 4, 73, 289, 227, 268, 297, 0, 253, 149, 240, 155, 65, 177, 276, 3, 268, 225, 155, 123, 63, 240, 4, 73, 191, 296, 33, 286, 200, 197, 159, 3, 203, 259, 211, 278, 3, 177, 114, 50, 84, 3, 268, 114, 63, 51, 3, 134, 268, 114, 91, 83, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(ids)\n"
      ],
      "metadata": {
        "id": "FNqd11VtfRtz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "d2789e38-d319-4a06-ab6e-70f67b923b89"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Ronaldo has won five Ballon d' Or awards, a record three UEFA Men' s Player of the Year Awards, and four European Golden Shoes. He was named the world' s best player by FIFA five times, the most by a European player. He has won 33 trophies in his career, including seven league titles, five UEFA Champions Leagues, the UEFA European Championship, and the UEFA Nations League.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34mPhCO7uAXX"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    \n",
        "Based on the output above, we can see that the decode method successfully converted the\n",
        "token IDs back into the original text.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOBn7yoWuAXY"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "So far, so good. We implemented a tokenizer capable of tokenizing and de-tokenizing\n",
        "text based on a snippet from the training set.\n",
        "\n",
        "Let's now apply it to a new text sample that\n",
        "is not contained in the training set:\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text = \"Hello, do you like tea?\"\n",
        "# print(tokenizer.encode(text))"
      ],
      "metadata": {
        "id": "Uu0NIETSgH-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBRW6u8LuAXY"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    \n",
        "The problem is that the word \"Hello\" was not used in the The Verdict short story.\n",
        "\n",
        "Hence, it\n",
        "is not contained in the vocabulary.\n",
        "\n",
        "This highlights the need to consider large and diverse\n",
        "training sets to extend the vocabulary when working on LLMs.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csY59MvKuAXY"
      },
      "source": [
        "### ADDING SPECIAL CONTEXT TOKENS\n",
        "\n",
        "In the previous section, we implemented a simple tokenizer and applied it to a passage\n",
        "from the training set.\n",
        "\n",
        "In this section, we will modify this tokenizer to handle unknown\n",
        "words.\n",
        "\n",
        "\n",
        "In particular, we will modify the vocabulary and tokenizer we implemented in the\n",
        "previous section, SimpleTokenizerV2, to support two new tokens, <|unk|> and\n",
        "<|endoftext|>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5GZIYmruAXY"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "We can modify the tokenizer to use an <|unk|> token if it\n",
        "encounters a word that is not part of the vocabulary.\n",
        "\n",
        "Furthermore, we add a token between\n",
        "unrelated texts.\n",
        "\n",
        "For example, when training GPT-like LLMs on multiple independent\n",
        "documents or books, it is common to insert a token before each document or book that\n",
        "follows a previous text source\n",
        "\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeP5w9zSuAXY"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Let's now modify the vocabulary to include these two special tokens, <unk> and\n",
        "<|endoftext|>, by adding these to the list of all unique words that we created in the\n",
        "previous section:\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sorted list of unique tokens from the preprocessed list\n",
        "# 1. `set(preprocessed)` creates a set of unique tokens from the `preprocessed` list.\n",
        "# 2. `list(set(preprocessed))` converts the set back into a list.\n",
        "# 3. `sorted()` sorts the list of unique tokens alphabetically.\n",
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "\n",
        "# Add special tokens to the list of all tokens\n",
        "# 1. `<|endoftext|>` is a special token used to mark the end of a text sequence.\n",
        "# 2. `<|unk|>` is a special token used to represent unknown or out-of-vocabulary tokens.\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "\n",
        "# Create a vocabulary dictionary that maps each token to a unique integer\n",
        "# 1. `enumerate(all_tokens)` generates pairs of (index, token) for each token in `all_tokens`.\n",
        "#    - `index` is the position of the token in the sorted list (starting from 0).\n",
        "#    - `token` is the word or special token itself.\n",
        "# 2. A dictionary comprehension is used to create the `vocab` dictionary:\n",
        "#    - The key is the `token` (word or special token).\n",
        "#    - The value is the `integer` (index).\n",
        "vocab = {token: integer for integer, token in enumerate(all_tokens)}"
      ],
      "metadata": {
        "id": "rqG0Zfoqg5LJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab.items())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlsSUzlK1Coz",
        "outputId": "fc24bbf7-90a3-44d7-9ca6-22b1435ea11b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "305"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86Y-wl-puAXY"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    \n",
        "Based on the output of the print statement above, the new vocabulary size is 305 (the\n",
        "vocabulary size in the previous section was 303).\n",
        "\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pEQde2buAXY"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "As an additional quick check, let's print the last 5 entries of the updated vocabulary:\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the last 5 items in the `vocab` dictionary\n",
        "# 1. `vocab.items()` returns a view of the dictionary's (token, integer) pairs.\n",
        "# 2. `list(vocab.items())` converts the view into a list of (token, integer) tuples.\n",
        "# 3. `[-5:]` slices the list to get the last 5 items.\n",
        "# 4. `enumerate()` adds an index to each item, generating pairs of (index, (token, integer)).\n",
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "    # Print the current item (tuple of (token, integer))\n",
        "    print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuyJ-xU-1Fpj",
        "outputId": "344b7dbd-51d6-4909-83f2-a705891e46fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('£88', 300)\n",
            "('€100', 301)\n",
            "('€94', 302)\n",
            "('<|endoftext|>', 303)\n",
            "('<|unk|>', 304)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p43GCzEwuAXZ"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "A simple text tokenizer that handles unknown words</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH3B_eQIuAXZ"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    \n",
        "Step 1: Replace unknown words by <|unk|> tokens\n",
        "    \n",
        "Step 2: Replace spaces before the specified punctuations\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        \"\"\"\n",
        "        Initialize the tokenizer with a vocabulary mapping.\n",
        "\n",
        "        Args:\n",
        "            vocab (dict): A dictionary mapping tokens (strings) to unique integers (IDs).\n",
        "        \"\"\"\n",
        "        # Map tokens to IDs (e.g., {\"Hello\": 1, \"world\": 2})\n",
        "        self.str_to_int = vocab\n",
        "\n",
        "        # Create the inverse mapping: IDs to tokens (e.g., {1: \"Hello\", 2: \"world\"})\n",
        "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"\n",
        "        Convert a text string into a list of token IDs.\n",
        "\n",
        "        Args:\n",
        "            text (str): The input text to encode.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of token IDs corresponding to the input text.\n",
        "        \"\"\"\n",
        "        import re\n",
        "\n",
        "        # Step 1: Preprocess the text\n",
        "        # Split the text into tokens and separators (like punctuation and spaces)\n",
        "        # using a regular expression.\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "        # Step 2: Clean the preprocessed list\n",
        "        # Remove leading/trailing spaces from each item and filter out empty strings.\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "\n",
        "        # Step 3: Handle unknown tokens\n",
        "        # Replace any token not in the vocabulary with the special \"<|unk|>\" token.\n",
        "        preprocessed = [\n",
        "            item if item in self.str_to_int\n",
        "            else \"<|unk|>\" for item in preprocessed\n",
        "        ]\n",
        "\n",
        "        # Step 4: Convert tokens to IDs\n",
        "        # Use the str_to_int mapping to convert each token to its corresponding ID.\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "\n",
        "        # Return the list of token IDs\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        \"\"\"\n",
        "        Convert a list of token IDs back into a text string.\n",
        "\n",
        "        Args:\n",
        "            ids (list): A list of token IDs to decode.\n",
        "\n",
        "        Returns:\n",
        "            str: The decoded text string.\n",
        "        \"\"\"\n",
        "        # Step 1: Convert IDs back to tokens\n",
        "        # Use the int_to_str mapping to convert each ID to its corresponding token.\n",
        "        tokens = [self.int_to_str[i] for i in ids]\n",
        "\n",
        "        # Step 2: Join tokens into a single string with spaces in between\n",
        "        text = \" \".join(tokens)\n",
        "\n",
        "        # Step 3: Postprocess the text\n",
        "        # Fix spacing issues around punctuation marks (e.g., \"Hello , world !\" -> \"Hello, world!\")\n",
        "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
        "\n",
        "        # Return the decoded text\n",
        "        return text"
      ],
      "metadata": {
        "id": "5ZP7d9901HOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tokenizer instance using the vocabulary\n",
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "\n",
        "# Define two example text strings\n",
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "\n",
        "# Combine the two text strings into a single string, separated by the special token \"<|endoftext|>\"\n",
        "# The `join()` method inserts \"<|endoftext|>\" between `text1` and `text2`.\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "\n",
        "# Print the combined text\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_k3chm-1IxA",
        "outputId": "136c8719-499e-492e-e63b-36d93d917cd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qw74XR1A1M29",
        "outputId": "9d3941e1-6199-42ac-e3cd-d2a16a22903f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[304, 3, 304, 304, 304, 304, 304, 303, 74, 268, 304, 304, 230, 268, 304, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "LxOEqT0s1NPA",
        "outputId": "22587ab0-f16f-47cf-9ab6-607b5c6cbeda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|unk|>, <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|endoftext|> In the <|unk|> <|unk|> of the <|unk|>.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY3oEamguAXZ"
      },
      "source": [
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "    \n",
        "Based on comparing the de-tokenized text above with the original input text, we know that\n",
        "the training dataset, Edith Wharton's short story The Verdict, did not contain the words\n",
        "\"Hello\" and \"palace.\"\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o12iPcWcuAXZ"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "So far, we have discussed tokenization as an essential step in processing text as input to\n",
        "LLMs. Depending on the LLM, some researchers also consider additional special tokens such\n",
        "as the following:\n",
        "\n",
        "[BOS] (beginning of sequence): This token marks the start of a text. It\n",
        "signifies to the LLM where a piece of content begins.\n",
        "\n",
        "[EOS] (end of sequence): This token is positioned at the end of a text,\n",
        "and is especially useful when concatenating multiple unrelated texts,\n",
        "similar to <|endoftext|>. For instance, when combining two different\n",
        "Wikipedia articles or books, the [EOS] token indicates where one article\n",
        "ends and the next one begins.\n",
        "\n",
        "[PAD] (padding): When training LLMs with batch sizes larger than one,\n",
        "the batch might contain texts of varying lengths. To ensure all texts have\n",
        "the same length, the shorter texts are extended or \"padded\" using the\n",
        "[PAD] token, up to the length of the longest text in the batch.\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZAYZ8xYuAXZ"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "Note that the tokenizer used for GPT models does not need any of these tokens mentioned\n",
        "above but only uses an <|endoftext|> token for simplicity\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDMwV1l-uAXa"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "the tokenizer used for GPT models also doesn't use an <|unk|> token for outof-vocabulary words. Instead, GPT models use a byte pair encoding tokenizer, which breaks\n",
        "down words into subword units\n",
        "</div>"
      ]
    }
  ]
}