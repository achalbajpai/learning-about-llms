{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbl32gCM+/dfgA0OfXUuu4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/achalbajpai/llm-scratch/blob/main/LLMsFromScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ib-eIsYGd221"
      },
      "source": [
        "## Step 1: Creating Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "The print command prints the total number of characters followed by the first 100\n",
        "characters of this file for illustration purposes. </div>"
      ],
      "metadata": {
        "id": "awnV7fdmkqw4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrQmKG-dV6E2",
        "outputId": "a20b764e-5870-4d3c-8288-e8abde7e7d6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of characters: 4348\n",
            "Cristiano Ronaldo dos Santos Aveiro, born on 5 February 1985, is a Portuguese professional football\n"
          ]
        }
      ],
      "source": [
        "# Open the file \"cristiano-ronaldo.txt\" in read mode with UTF-8 encoding\n",
        "# The `with` statement ensures the file is properly closed after reading\n",
        "with open(\"cr7.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    # Read the entire content of the file into the variable `raw_text`\n",
        "    raw_text = f.read()\n",
        "\n",
        "# Print the total number of characters in the file\n",
        "# `len(raw_text)` returns the length of the string `raw_text`, which is the number of characters\n",
        "print(\"Total number of characters:\", len(raw_text))\n",
        "\n",
        "# Print the first 99 characters of the file\n",
        "# `raw_text[:99]` slices the string `raw_text` to get the first 99 characters\n",
        "print(raw_text[:99])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer\n"
      ],
      "metadata": {
        "id": "bpU_tvTHfelh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Our goal is to tokenize this 4348-character short story into individual words and special\n",
        "characters that we can then turn into embeddings for LLM training  </div>"
      ],
      "metadata": {
        "id": "rSQyI89-lu9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "Note that it's common to process millions of articles and hundreds of thousands of\n",
        "books -- many gigabytes of text -- when working with LLMs. However, for educational\n",
        "purposes, it's sufficient to work with smaller text samples like a single book to\n",
        "illustrate the main ideas behind the text processing steps and to make it possible to\n",
        "run it in reasonable time on consumer hardware. </div>"
      ],
      "metadata": {
        "id": "6Md5PglPlz3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the `re` module, which provides support for regular expressions in Python\n",
        "import re\n",
        "\n",
        "# Define a sample text string to be processed\n",
        "text = \"Hello, world. This, is a test.\"\n",
        "\n",
        "# Use the `re.split()` function to split the text into tokens and separators\n",
        "# The regular expression `([,.:;?_!\"()\\']|--|\\s)` matches:\n",
        "# - Punctuation marks: , . : ; ? _ ! \" ( ) '\n",
        "# - Double hyphens: --\n",
        "# - Whitespace: \\s\n",
        "# The parentheses `()` around the regex ensure that the separators are included in the result\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "# Print the initial result of the split\n",
        "print(result)\n",
        "\n",
        "# Clean the result list:\n",
        "# 1. Use a list comprehension to iterate over each item in `result`\n",
        "# 2. Apply `item.strip()` to remove leading and trailing whitespace from each item\n",
        "# 3. Filter out any empty strings using `if item.strip()`\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "\n",
        "# Print the cleaned result\n",
        "print(result)\n",
        "\n",
        "# Alternative approach (redundant in this case, as the previous step already handles this):\n",
        "# Filter out any empty strings from the list\n",
        "# This step is unnecessary here because the previous list comprehension already removes empty strings\n",
        "result = [item for item in result if item.strip()]\n",
        "\n",
        "# Print the final result\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsjZSf8vd3hr",
        "outputId": "9211f613-2afc-4b53-9066-ca2cb0027e82"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n",
            "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n",
            "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the `re.split()` function to split the `raw_text` string into tokens and separators\n",
        "# The regular expression `([,.:;?_!\"()\\']|--|\\s)` matches:\n",
        "# - Punctuation marks: , . : ; ? _ ! \" ( ) '\n",
        "# - Double hyphens: --\n",
        "# - Whitespace: \\s\n",
        "# The parentheses `()` around the regex ensure that the separators are included in the result\n",
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "\n",
        "# Clean the `preprocessed` list:\n",
        "# 1. Use a list comprehension to iterate over each item in `preprocessed`\n",
        "# 2. Apply `item.strip()` to remove leading and trailing whitespace from each item\n",
        "# 3. Filter out any empty strings using `if item.strip()`\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "\n",
        "# Print the first 30 items of the cleaned `preprocessed` list\n",
        "# This is useful for inspecting the initial part of the processed data\n",
        "print(preprocessed[:30])\n",
        "\n",
        "# Print the total number of items in the cleaned `preprocessed` list\n",
        "# This gives an idea of the size of the processed data\n",
        "print(len(preprocessed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkIWCZXFfqzF",
        "outputId": "d0f499a8-5176-4573-ac3d-714fc3ae9bec"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Cristiano', 'Ronaldo', 'dos', 'Santos', 'Aveiro', ',', 'born', 'on', '5', 'February', '1985', ',', 'is', 'a', 'Portuguese', 'professional', 'footballer', '.', 'He', 'plays', 'as', 'a', 'forward', 'for', 'and', 'captains', 'both', 'the', 'Saudi', 'Pro']\n",
            "882\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2 - Token to Token ID**\n"
      ],
      "metadata": {
        "id": "QD-LNBrRgidF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sorted list of unique words (tokens) from the `preprocessed` list\n",
        "# 1. `set(preprocessed)` creates a set of unique items from `preprocessed`.\n",
        "# 2. `sorted()` sorts the unique items alphabetically.\n",
        "all_words = sorted(set(preprocessed))\n",
        "\n",
        "# Calculate the size of the vocabulary (number of unique words)\n",
        "# `len(all_words)` returns the number of items in the `all_words` list\n",
        "vocab_size = len(all_words)\n",
        "\n",
        "# Print the size of the vocabulary\n",
        "# This gives the total number of unique words in the preprocessed data\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwYfMtxBgpvs",
        "outputId": "2238823f-fa2b-41e3-afc5-2686e5beb5a4"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a vocabulary dictionary that maps each unique token to a unique integer\n",
        "# 1. `enumerate(all_words)` generates pairs of (index, token) for each token in `all_words`.\n",
        "#    - `index` is the position of the token in the sorted list (starting from 0).\n",
        "#    - `token` is the word itself.\n",
        "# 2. A dictionary comprehension is used to create the `vocab` dictionary:\n",
        "#    - The key is the `token` (word).\n",
        "#    - The value is the `integer` (index).\n",
        "vocab = {token: integer for integer, token in enumerate(all_words)}"
      ],
      "metadata": {
        "id": "KzZF8CuBhsWH"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the items in the `vocab` dictionary using `enumerate`\n",
        "# `enumerate(vocab.items())` generates pairs of (index, (token, integer)) for each item in `vocab`\n",
        "# - `index` is the position of the item in the iteration (starting from 0).\n",
        "# - `item` is a tuple of (token, integer), where:\n",
        "#   - `token` is the word (key in the dictionary).\n",
        "#   - `integer` is the corresponding ID (value in the dictionary).\n",
        "for i, item in enumerate(vocab.items()):\n",
        "    # Print the current item (tuple of (token, integer))\n",
        "    print(item)\n",
        "\n",
        "    # Stop the loop after printing 50 items\n",
        "    # This is useful for inspecting the first few items in a large dictionary\n",
        "    if i >= 50:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JraDFZHjh2L1",
        "outputId": "68f15488-edf3-4346-cf56-36dc7a1e41b5"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"'\", 0)\n",
            "('(', 1)\n",
            "(')', 2)\n",
            "(',', 3)\n",
            "('.', 4)\n",
            "('1', 5)\n",
            "('100', 6)\n",
            "('135', 7)\n",
            "('14', 8)\n",
            "('140', 9)\n",
            "('18', 10)\n",
            "('183', 11)\n",
            "('1985', 12)\n",
            "('200', 13)\n",
            "('2003', 14)\n",
            "('2004', 15)\n",
            "('2008', 16)\n",
            "('2009', 17)\n",
            "('2013', 18)\n",
            "('2014', 19)\n",
            "('2015', 20)\n",
            "('2016', 21)\n",
            "('2017', 22)\n",
            "('2018', 23)\n",
            "('2019', 24)\n",
            "('2020', 25)\n",
            "('2021', 26)\n",
            "('2022', 27)\n",
            "('2023', 28)\n",
            "('2024', 29)\n",
            "('217', 30)\n",
            "('23', 31)\n",
            "('30', 32)\n",
            "('33', 33)\n",
            "('42', 34)\n",
            "('5', 35)\n",
            "('8', 36)\n",
            "('900', 37)\n",
            "('A', 38)\n",
            "('Additionally', 39)\n",
            "('Al', 40)\n",
            "('At', 41)\n",
            "('Aveiro', 42)\n",
            "('Awards', 43)\n",
            "('Bale', 44)\n",
            "('Ballon', 45)\n",
            "('Ballons', 46)\n",
            "('Benzema', 47)\n",
            "('Boot', 48)\n",
            "('CP', 49)\n",
            "('Champions', 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        \"\"\"\n",
        "        Initialize the tokenizer with a vocabulary mapping.\n",
        "\n",
        "        Args:\n",
        "            vocab (dict): A dictionary mapping tokens (strings) to unique integers (IDs).\n",
        "        \"\"\"\n",
        "        # Map tokens to IDs (e.g., {\"Hello\": 1, \"world\": 2})\n",
        "        self.str_to_int = vocab\n",
        "\n",
        "        # Create the inverse mapping: IDs to tokens (e.g., {1: \"Hello\", 2: \"world\"})\n",
        "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"\n",
        "        Convert a text string into a list of token IDs.\n",
        "\n",
        "        Args:\n",
        "            text (str): The input text to encode.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of token IDs corresponding to the input text.\n",
        "        \"\"\"\n",
        "        import re\n",
        "\n",
        "        # Step 1: Preprocess the text\n",
        "        # Split the text into tokens and separators (like punctuation and spaces)\n",
        "        # using a regular expression.\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "        # Step 2: Clean the preprocessed list\n",
        "        # Remove leading/trailing spaces from each item and filter out empty strings.\n",
        "        preprocessed = [\n",
        "            item.strip() for item in preprocessed if item.strip()\n",
        "        ]\n",
        "\n",
        "        # Step 3: Convert tokens to IDs\n",
        "        # Use the str_to_int mapping to convert each token to its corresponding ID.\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "\n",
        "        # Return the list of token IDs\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        \"\"\"\n",
        "        Convert a list of token IDs back into a text string.\n",
        "\n",
        "        Args:\n",
        "            ids (list): A list of token IDs to decode.\n",
        "\n",
        "        Returns:\n",
        "            str: The decoded text string.\n",
        "        \"\"\"\n",
        "        # Step 1: Convert IDs back to tokens\n",
        "        # Use the int_to_str mapping to convert each ID to its corresponding token.\n",
        "        tokens = [self.int_to_str[i] for i in ids]\n",
        "\n",
        "        # Step 2: Join tokens into a single string with spaces in between\n",
        "        text = \" \".join(tokens)\n",
        "\n",
        "        # Step 3: Postprocess the text\n",
        "        # Fix spacing issues around punctuation marks (e.g., \"Hello , world !\" -> \"Hello, world!\")\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "\n",
        "        # Return the decoded text\n",
        "        return text"
      ],
      "metadata": {
        "id": "L83OMlkWeHwI"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "# Input text\n",
        "text = \"\"\"Ronaldo has won five Ballon d'Or awards, a record three UEFA Men's Player of the Year Awards, and four European Golden Shoes. He was named the world's best player by FIFA five times, the most by a European player. He has won 33 trophies in his career, including seven league titles, five UEFA Champions Leagues, the UEFA European Championship, and the UEFA Nations League.\"\"\"\n",
        "\n",
        "# Encode the text\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "id": "I8OP_UplebPg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94d9acf0-819d-41eb-981a-fba4168bcd76"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100, 191, 296, 177, 45, 163, 0, 92, 144, 3, 123, 249, 273, 114, 88, 0, 253, 93, 230, 268, 121, 43, 3, 134, 184, 63, 72, 104, 4, 73, 289, 227, 268, 297, 0, 253, 149, 240, 155, 65, 177, 276, 3, 268, 225, 155, 123, 63, 240, 4, 73, 191, 296, 33, 286, 200, 197, 159, 3, 203, 259, 211, 278, 3, 177, 114, 50, 84, 3, 268, 114, 63, 51, 3, 134, 268, 114, 91, 83, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(ids)\n"
      ],
      "metadata": {
        "id": "FNqd11VtfRtz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "f52e907a-ea2d-4fda-98a7-c2566b101416"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Ronaldo has won five Ballon d' Or awards, a record three UEFA Men' s Player of the Year Awards, and four European Golden Shoes. He was named the world' s best player by FIFA five times, the most by a European player. He has won 33 trophies in his career, including seven league titles, five UEFA Champions Leagues, the UEFA European Championship, and the UEFA Nations League.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text = \"Hello, do you like tea?\"\n",
        "# print(tokenizer.encode(text))"
      ],
      "metadata": {
        "id": "Uu0NIETSgH-d"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SPECIAL TOKENS**"
      ],
      "metadata": {
        "id": "LD-dwSWqgo9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sorted list of unique tokens from the preprocessed list\n",
        "# 1. `set(preprocessed)` creates a set of unique tokens from the `preprocessed` list.\n",
        "# 2. `list(set(preprocessed))` converts the set back into a list.\n",
        "# 3. `sorted()` sorts the list of unique tokens alphabetically.\n",
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "\n",
        "# Add special tokens to the list of all tokens\n",
        "# 1. `<|endoftext|>` is a special token used to mark the end of a text sequence.\n",
        "# 2. `<|unk|>` is a special token used to represent unknown or out-of-vocabulary tokens.\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "\n",
        "# Create a vocabulary dictionary that maps each token to a unique integer\n",
        "# 1. `enumerate(all_tokens)` generates pairs of (index, token) for each token in `all_tokens`.\n",
        "#    - `index` is the position of the token in the sorted list (starting from 0).\n",
        "#    - `token` is the word or special token itself.\n",
        "# 2. A dictionary comprehension is used to create the `vocab` dictionary:\n",
        "#    - The key is the `token` (word or special token).\n",
        "#    - The value is the `integer` (index).\n",
        "vocab = {token: integer for integer, token in enumerate(all_tokens)}"
      ],
      "metadata": {
        "id": "rqG0Zfoqg5LJ"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab.items())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlsSUzlK1Coz",
        "outputId": "fc24bbf7-90a3-44d7-9ca6-22b1435ea11b"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "305"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the last 5 items in the `vocab` dictionary\n",
        "# 1. `vocab.items()` returns a view of the dictionary's (token, integer) pairs.\n",
        "# 2. `list(vocab.items())` converts the view into a list of (token, integer) tuples.\n",
        "# 3. `[-5:]` slices the list to get the last 5 items.\n",
        "# 4. `enumerate()` adds an index to each item, generating pairs of (index, (token, integer)).\n",
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "    # Print the current item (tuple of (token, integer))\n",
        "    print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuyJ-xU-1Fpj",
        "outputId": "344b7dbd-51d6-4909-83f2-a705891e46fd"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('£88', 300)\n",
            "('€100', 301)\n",
            "('€94', 302)\n",
            "('<|endoftext|>', 303)\n",
            "('<|unk|>', 304)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        \"\"\"\n",
        "        Initialize the tokenizer with a vocabulary mapping.\n",
        "\n",
        "        Args:\n",
        "            vocab (dict): A dictionary mapping tokens (strings) to unique integers (IDs).\n",
        "        \"\"\"\n",
        "        # Map tokens to IDs (e.g., {\"Hello\": 1, \"world\": 2})\n",
        "        self.str_to_int = vocab\n",
        "\n",
        "        # Create the inverse mapping: IDs to tokens (e.g., {1: \"Hello\", 2: \"world\"})\n",
        "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"\n",
        "        Convert a text string into a list of token IDs.\n",
        "\n",
        "        Args:\n",
        "            text (str): The input text to encode.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of token IDs corresponding to the input text.\n",
        "        \"\"\"\n",
        "        import re\n",
        "\n",
        "        # Step 1: Preprocess the text\n",
        "        # Split the text into tokens and separators (like punctuation and spaces)\n",
        "        # using a regular expression.\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "        # Step 2: Clean the preprocessed list\n",
        "        # Remove leading/trailing spaces from each item and filter out empty strings.\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "\n",
        "        # Step 3: Handle unknown tokens\n",
        "        # Replace any token not in the vocabulary with the special \"<|unk|>\" token.\n",
        "        preprocessed = [\n",
        "            item if item in self.str_to_int\n",
        "            else \"<|unk|>\" for item in preprocessed\n",
        "        ]\n",
        "\n",
        "        # Step 4: Convert tokens to IDs\n",
        "        # Use the str_to_int mapping to convert each token to its corresponding ID.\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "\n",
        "        # Return the list of token IDs\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        \"\"\"\n",
        "        Convert a list of token IDs back into a text string.\n",
        "\n",
        "        Args:\n",
        "            ids (list): A list of token IDs to decode.\n",
        "\n",
        "        Returns:\n",
        "            str: The decoded text string.\n",
        "        \"\"\"\n",
        "        # Step 1: Convert IDs back to tokens\n",
        "        # Use the int_to_str mapping to convert each ID to its corresponding token.\n",
        "        tokens = [self.int_to_str[i] for i in ids]\n",
        "\n",
        "        # Step 2: Join tokens into a single string with spaces in between\n",
        "        text = \" \".join(tokens)\n",
        "\n",
        "        # Step 3: Postprocess the text\n",
        "        # Fix spacing issues around punctuation marks (e.g., \"Hello , world !\" -> \"Hello, world!\")\n",
        "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
        "\n",
        "        # Return the decoded text\n",
        "        return text"
      ],
      "metadata": {
        "id": "5ZP7d9901HOj"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tokenizer instance using the vocabulary\n",
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "\n",
        "# Define two example text strings\n",
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "\n",
        "# Combine the two text strings into a single string, separated by the special token \"<|endoftext|>\"\n",
        "# The `join()` method inserts \"<|endoftext|>\" between `text1` and `text2`.\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "\n",
        "# Print the combined text\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_k3chm-1IxA",
        "outputId": "136c8719-499e-492e-e63b-36d93d917cd9"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qw74XR1A1M29",
        "outputId": "9d3941e1-6199-42ac-e3cd-d2a16a22903f"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[304, 3, 304, 304, 304, 304, 304, 303, 74, 268, 304, 304, 230, 268, 304, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "LxOEqT0s1NPA",
        "outputId": "22587ab0-f16f-47cf-9ab6-607b5c6cbeda"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|unk|>, <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|endoftext|> In the <|unk|> <|unk|> of the <|unk|>.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    }
  ]
}